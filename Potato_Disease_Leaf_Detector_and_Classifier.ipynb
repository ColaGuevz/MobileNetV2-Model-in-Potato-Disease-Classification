{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MobileNetV2 Model"
      ],
      "metadata": {
        "id": "tCfquvSpotcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "dataset_dir = \"/content/drive/MyDrive/Potato Disease Leaf Dataset\"\n",
        "train_dir = os.path.join(dataset_dir, \"Training\")\n",
        "val_dir   = os.path.join(dataset_dir, \"Validation\")\n",
        "test_dir  = os.path.join(dataset_dir, \"Testing\")\n",
        "\n",
        "# Parameters\n",
        "img_size = (224, 224)  # MobileNetV2 default is 224x224\n",
        "batch_size = 32\n",
        "\n",
        "# Data Augmentation for Training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# For validation and test, just preprocess\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# Generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Load MobileNetV2 base (exclude top classification layers)\n",
        "base_model = MobileNetV2(input_shape=(224, 224, 3),\n",
        "                         include_top=False,\n",
        "                         weights='imagenet')\n",
        "\n",
        "# Freeze base_model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build a new top (classification head)\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(3, activation='softmax')(x)  # 3 classes\n",
        "\n",
        "model = models.Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-3),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train only the new top layers\n",
        "epochs = 10\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Optionally, unfreeze some top layers of the base_model for fine-tuning\n",
        "# For example:\n",
        "for layer in base_model.layers[-30:]:  # Unfreeze the last 30 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with a lower learning rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Continue training\n",
        "fine_tune_epochs = 10\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=fine_tune_epochs,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test Accuracy after Transfer Learning: {test_acc:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model.save(\"/content/drive/MyDrive/Potato Disease Leaf CNN Model/potato_leaf_mobilenetv2.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuYEByyio1VU",
        "outputId": "f7c4d03a-9e6e-4f43-95ab-f00645f0c4db"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 3251 images belonging to 3 classes.\n",
            "Found 416 images belonging to 3 classes.\n",
            "Found 415 images belonging to 3 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 573ms/step - accuracy: 0.7754 - loss: 0.5609 - val_accuracy: 0.9423 - val_loss: 0.1576\n",
            "Epoch 2/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 515ms/step - accuracy: 0.9151 - loss: 0.2387 - val_accuracy: 0.9567 - val_loss: 0.1288\n",
            "Epoch 3/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 507ms/step - accuracy: 0.9440 - loss: 0.1695 - val_accuracy: 0.9736 - val_loss: 0.0998\n",
            "Epoch 4/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 501ms/step - accuracy: 0.9527 - loss: 0.1337 - val_accuracy: 0.9591 - val_loss: 0.1223\n",
            "Epoch 5/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 497ms/step - accuracy: 0.9593 - loss: 0.1364 - val_accuracy: 0.9760 - val_loss: 0.0961\n",
            "Epoch 6/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 522ms/step - accuracy: 0.9513 - loss: 0.1204 - val_accuracy: 0.9615 - val_loss: 0.1283\n",
            "Epoch 7/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 499ms/step - accuracy: 0.9525 - loss: 0.1502 - val_accuracy: 0.9471 - val_loss: 0.1401\n",
            "Epoch 8/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 507ms/step - accuracy: 0.9569 - loss: 0.1316 - val_accuracy: 0.9760 - val_loss: 0.0976\n",
            "Epoch 9/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 496ms/step - accuracy: 0.9575 - loss: 0.1209 - val_accuracy: 0.9784 - val_loss: 0.0849\n",
            "Epoch 10/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 520ms/step - accuracy: 0.9636 - loss: 0.0964 - val_accuracy: 0.9423 - val_loss: 0.1717\n",
            "Epoch 1/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 598ms/step - accuracy: 0.9208 - loss: 0.2435 - val_accuracy: 0.9159 - val_loss: 0.2672\n",
            "Epoch 2/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 507ms/step - accuracy: 0.9630 - loss: 0.0934 - val_accuracy: 0.9111 - val_loss: 0.3131\n",
            "Epoch 3/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 526ms/step - accuracy: 0.9767 - loss: 0.0593 - val_accuracy: 0.9183 - val_loss: 0.3708\n",
            "Epoch 4/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 502ms/step - accuracy: 0.9780 - loss: 0.0635 - val_accuracy: 0.9231 - val_loss: 0.4092\n",
            "Epoch 5/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 504ms/step - accuracy: 0.9810 - loss: 0.0451 - val_accuracy: 0.9639 - val_loss: 0.1778\n",
            "Epoch 6/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 506ms/step - accuracy: 0.9884 - loss: 0.0343 - val_accuracy: 0.9471 - val_loss: 0.2926\n",
            "Epoch 7/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 504ms/step - accuracy: 0.9869 - loss: 0.0429 - val_accuracy: 0.9712 - val_loss: 0.1194\n",
            "Epoch 8/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 510ms/step - accuracy: 0.9922 - loss: 0.0189 - val_accuracy: 0.9736 - val_loss: 0.1046\n",
            "Epoch 9/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 506ms/step - accuracy: 0.9943 - loss: 0.0193 - val_accuracy: 0.9255 - val_loss: 0.4003\n",
            "Epoch 10/10\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 503ms/step - accuracy: 0.9875 - loss: 0.0385 - val_accuracy: 0.9760 - val_loss: 0.1035\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 422ms/step - accuracy: 0.9735 - loss: 0.1327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy after Transfer Learning: 0.9663\n"
          ]
        }
      ]
    }
  ]
}